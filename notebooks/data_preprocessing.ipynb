{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb00d9e1",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Electric Load Forecasting\n",
    "\n",
    "This notebook demonstrates a production-style data preprocessing pipeline for electric load forecasting. We combine electricity demand and weather data for ten major U.S. cities, ensuring robust merging, cleaning, feature engineering, aggregation, and anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5778ce",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "261778de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(97892) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.2.5)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d34ead33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "import re\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980f3fa",
   "metadata": {},
   "source": [
    "## 1. Loading & Inspection\n",
    "\n",
    "We load and merge all CSV files for the ten cities into a single unified dataset. City names are standardized (canonicalized) and timestamps are converted to datetime objects. This ensures that, for example, 'New York', 'NEW YORK', and 'new york' are all treated as 'new_york'. We then review the schema and sample records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5919bacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demand data shape: (199429, 3)\n",
      "                 timestamp    city    demand\n",
      "205077 2018-01-01 00:00:00  dallas  18346.96\n",
      "205054 2018-01-01 01:00:00  dallas  18584.34\n",
      "205055 2018-01-01 02:00:00  dallas  18524.14\n",
      "205056 2018-01-01 03:00:00  dallas  18532.06\n",
      "205057 2018-01-01 04:00:00  dallas  18647.44\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./dataset/\"\n",
    "\n",
    "def canonical(city: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \"_\", city.strip().lower())\n",
    "\n",
    "demand_frames = []\n",
    "for f in [\"cleaned_balance_data.csv\", \"cleaned_subregion_data.csv\"]:\n",
    "    file_path = os.path.join(dataset_path, f)\n",
    "    if os.path.exists(file_path):\n",
    "        dataset = pd.read_csv(file_path)\n",
    "        time_col = 'local_time' if 'local_time' in dataset.columns else 'utc_time'\n",
    "        dataset['timestamp'] = pd.to_datetime(dataset[time_col])\n",
    "        dataset['city'] = dataset['city'].map(canonical)\n",
    "        demand_frames.append(dataset[[\"timestamp\", \"city\", \"demand\"]])\n",
    "\n",
    "tx_path = os.path.join(dataset_path, \"cleaned_texas_data.csv\")\n",
    "if os.path.exists(tx_path):\n",
    "    tx = pd.read_csv(tx_path)\n",
    "    tx = (tx.rename(columns={\"date\": \"timestamp\"})\n",
    "          .melt(id_vars=\"timestamp\", var_name=\"city\", value_name=\"demand\"))\n",
    "    tx['timestamp'] = pd.to_datetime(tx['timestamp'])\n",
    "    tx['city'] = tx['city'].map(canonical)\n",
    "    demand_frames.append(tx)\n",
    "\n",
    "demand = pd.concat(demand_frames, ignore_index=True).dropna(subset=[\"demand\"]).sort_values([\"city\", \"timestamp\"])\n",
    "print(\"Demand data shape:\", demand.shape)\n",
    "print(demand.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103d2129",
   "metadata": {},
   "source": [
    "## 2. Merging Weather Data\n",
    "\n",
    "We load weather data from JSON files for each city, standardize city names, and convert timestamps. We then merge the demand and weather data on both 'city' and 'timestamp' using an inner join. This ensures city-specific matching, prevents cross-location data mixing, and avoids data multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dd291df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data shape: (186142, 9)\n",
      "            timestamp    city    demand  temperature  humidity  windSpeed  \\\n",
      "0 2018-07-02 05:00:00  dallas  13839.70        89.56      0.45      10.84   \n",
      "1 2018-07-02 06:00:00  dallas  14067.03        88.35      0.52       9.94   \n",
      "2 2018-07-02 07:00:00  dallas  14323.09        87.05      0.56       8.92   \n",
      "3 2018-07-02 08:00:00  dallas  14652.59        86.04      0.58       8.47   \n",
      "4 2018-07-02 09:00:00  dallas  15582.09        84.80      0.63       5.61   \n",
      "\n",
      "   pressure  precipIntensity  precipProbability  \n",
      "0    1011.9              0.0                0.0  \n",
      "1    1012.7              0.0                0.0  \n",
      "2    1012.8              0.0                0.0  \n",
      "3    1012.7              0.0                0.0  \n",
      "4    1012.9              0.0                0.0  \n"
     ]
    }
   ],
   "source": [
    "def load_weather(path):\n",
    "    arr = json.load(open(path))\n",
    "    df = pd.DataFrame(arr)\n",
    "    for col in ['time', 'timestamp', 'date']:\n",
    "        if col in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df[col], unit='s' if col == 'time' else None)\n",
    "            break\n",
    "    df['city'] = canonical(os.path.splitext(os.path.basename(path))[0])\n",
    "    keep = [\"timestamp\", \"city\", \"temperature\", \"humidity\", \"windSpeed\", \"pressure\", \"precipIntensity\", \"precipProbability\"]\n",
    "    return df[[c for c in keep if c in df.columns]]\n",
    "\n",
    "weather_files = glob(os.path.join(dataset_path, \"*.json\"))\n",
    "weather = pd.concat([load_weather(p) for p in weather_files], ignore_index=True)\n",
    "\n",
    "merged_data = pd.merge(demand, weather, on=[\"city\", \"timestamp\"], how=\"inner\")\n",
    "print(\"Merged data shape:\", merged_data.shape)\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee5dc1c",
   "metadata": {},
   "source": [
    "## 3. Missing Values\n",
    "\n",
    "We identify missing values and impute them using the median for each city, preserving local patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffa7ff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation:\n",
      "timestamp            0\n",
      "city                 0\n",
      "demand               0\n",
      "temperature          0\n",
      "humidity             0\n",
      "windSpeed            0\n",
      "pressure             0\n",
      "precipIntensity      0\n",
      "precipProbability    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for col in [\"temperature\", \"humidity\", \"windSpeed\", \"pressure\", \"precipIntensity\", \"precipProbability\"]:\n",
    "    merged_data[col] = merged_data.groupby(\"city\")[col].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "print(\"Missing values after imputation:\")\n",
    "print(merged_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3670d26d",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "We extract time-based features (hour, day of week, month, season) and normalize continuous variables using min-max scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51f65e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp    city    demand  hour  dayofweek  month  season\n",
      "0 2018-07-02 05:00:00  dallas  13839.70     5          0      7  summer\n",
      "1 2018-07-02 06:00:00  dallas  14067.03     6          0      7  summer\n",
      "2 2018-07-02 07:00:00  dallas  14323.09     7          0      7  summer\n",
      "3 2018-07-02 08:00:00  dallas  14652.59     8          0      7  summer\n",
      "4 2018-07-02 09:00:00  dallas  15582.09     9          0      7  summer\n"
     ]
    }
   ],
   "source": [
    "merged_data['hour'] = merged_data['timestamp'].dt.hour\n",
    "merged_data['dayofweek'] = merged_data['timestamp'].dt.dayofweek\n",
    "merged_data['month'] = merged_data['timestamp'].dt.month\n",
    "merged_data['season'] = merged_data['month'].apply(lambda x: 'winter' if x in [12, 1, 2] else 'spring' if x in [3, 4, 5] else 'summer' if x in [6, 7, 8] else 'autumn')\n",
    "\n",
    "for col in [\"demand\", \"temperature\", \"humidity\", \"windSpeed\", \"pressure\", \"precipIntensity\", \"precipProbability\"]:\n",
    "    min_val = merged_data[col].min()\n",
    "    max_val = merged_data[col].max()\n",
    "    merged_data[f\"{col}_scaled\"] = (merged_data[col] - min_val) / (max_val - min_val)\n",
    "\n",
    "print(merged_data[[\"timestamp\", \"city\", \"demand\", \"hour\", \"dayofweek\", \"month\", \"season\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b965dab1",
   "metadata": {},
   "source": [
    "## 5. Aggregation\n",
    "\n",
    "We compute daily and weekly summary statistics for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "123f0d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     city        date    avg_demand  max_demand  min_demand   avg_temp\n",
      "0  dallas  2018-07-02  19853.496316    24432.36    13839.70  89.716316\n",
      "1  dallas  2018-07-03  18955.707083    24431.58    13628.22  91.135833\n",
      "2  dallas  2018-07-04  16585.507500    20619.83    12494.99  85.995000\n",
      "3  dallas  2018-07-05  17137.723750    22809.88    11593.80  84.035833\n",
      "4  dallas  2018-07-06  16987.939167    21975.81    13094.74  86.249167\n",
      "     city  week    avg_demand   avg_temp\n",
      "0  dallas     1  12788.683532  48.238274\n",
      "1  dallas     2  12418.669603  52.366647\n",
      "2  dallas     3  12363.146210  52.670536\n",
      "3  dallas     4  13080.827024  48.068413\n",
      "4  dallas     5  12597.979841  50.075933\n"
     ]
    }
   ],
   "source": [
    "merged_data['date'] = merged_data['timestamp'].dt.date\n",
    "merged_data['week'] = merged_data['timestamp'].dt.isocalendar().week\n",
    "\n",
    "daily_summary = merged_data.groupby([\"city\", \"date\"]).agg(avg_demand=(\"demand\", \"mean\"), max_demand=(\"demand\", \"max\"), min_demand=(\"demand\", \"min\"), avg_temp=(\"temperature\", \"mean\")).reset_index()\n",
    "weekly_summary = merged_data.groupby([\"city\", \"week\"]).agg(avg_demand=(\"demand\", \"mean\"), avg_temp=(\"temperature\", \"mean\")).reset_index()\n",
    "\n",
    "print(daily_summary.head())\n",
    "print(weekly_summary.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd07924",
   "metadata": {},
   "source": [
    "## 6. Anomaly & Error Detection\n",
    "\n",
    "We use the entire dataset to uncover outliers and errors (e.g., sudden consumption spikes, impossible weather values). We apply Z-score, IQR, and Isolation Forest to flag anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb9eaea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies (Z-score): 3809\n",
      "Anomalies (IQR): 8657\n",
      "Anomalies (Isolation Forest): 1862\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "z_scores = merged_data[[\"demand\", \"temperature\", \"humidity\", \"windSpeed\", \"pressure\"]].apply(zscore)\n",
    "merged_data['anomaly_z'] = (np.abs(z_scores) > 3).any(axis=1)\n",
    "\n",
    "Q1 = merged_data[[\"demand\", \"temperature\", \"humidity\", \"windSpeed\", \"pressure\"]].quantile(0.25)\n",
    "Q3 = merged_data[[\"demand\", \"temperature\", \"humidity\", \"windSpeed\", \"pressure\"]].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outlier_condition = ((merged_data[[\"demand\", \"temperature\", \"humidity\", \"windSpeed\", \"pressure\"]] < (Q1 - 1.5 * IQR)) | (merged_data[[\"demand\", \"temperature\", \"humidity\", \"windSpeed\", \"pressure\"]] > (Q3 + 1.5 * IQR)))\n",
    "merged_data['anomaly_iqr'] = outlier_condition.any(axis=1)\n",
    "\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
    "features = [\"demand\", \"temperature\", \"humidity\", \"windSpeed\", \"pressure\"]\n",
    "merged_data['anomaly_iso'] = iso_forest.fit_predict(merged_data[features]) == -1\n",
    "\n",
    "print(\"Anomalies (Z-score):\", merged_data['anomaly_z'].sum())\n",
    "print(\"Anomalies (IQR):\", merged_data['anomaly_iqr'].sum())\n",
    "print(\"Anomalies (Isolation Forest):\", merged_data['anomaly_iso'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac7ac9",
   "metadata": {},
   "source": [
    "## 7. Output Processed Datasets\n",
    "\n",
    "We save the processed datasets for downstream clustering and forecasting.\n",
    "\n",
    "- `processed/load_data.csv`: Main processed dataset\n",
    "- `processed/daily_data.csv`: Daily summary\n",
    "- `processed/weekly_data.csv`: Weekly summary\n",
    "- `processed/clean_merged_data.csv`: Clean merged data before anomaly handling\n",
    "\n",
    "These outputs are ready for use in clustering and forecasting notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee36a7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed datasets saved.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.join(dataset_path, \"processed\"), exist_ok=True)\n",
    "merged_data.to_csv(os.path.join(dataset_path, \"processed\", \"load_data.csv\"), index=False)\n",
    "daily_summary.to_csv(os.path.join(dataset_path, \"processed\", \"daily_data.csv\"), index=False)\n",
    "weekly_summary.to_csv(os.path.join(dataset_path, \"processed\", \"weekly_data.csv\"), index=False)\n",
    "merged_data.to_csv(os.path.join(dataset_path, \"processed\", \"clean_merged_data.csv\"), index=False)\n",
    "\n",
    "print(\"Processed datasets saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdca59b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Summary:**\n",
    "Data is merged on canonical city names and timestamps using an inner join, ensuring city-specific matching and data integrity. All key preprocessing steps are documented and the resulting datasets are ready for clustering and forecasting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
