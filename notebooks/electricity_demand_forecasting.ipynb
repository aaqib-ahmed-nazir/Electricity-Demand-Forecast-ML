{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac028061",
   "metadata": {},
   "source": [
    "# âš¡ Electricity Demand Forecasting\n",
    "\n",
    " **Electricity Demand Forecasting** notebook! This notebook demonstrates a complete workflow for forecasting electricity demand using:\n",
    "- **Naive Baseline**\n",
    "- **XGBoost**\n",
    "- **LSTM Neural Network**\n",
    "- **Ensemble Model**\n",
    "\n",
    "You'll learn how to load and preprocess data, train and evaluate models, and compare their performance visually. Each step is clearly marked and explained for easy navigation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706fa731",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3166260c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ts/5lrll2nx1511bvwlbt5yh_940000gn/T/ipykernel_58550/604527073.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import xgboost as xgb\n",
    "import math\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import joblib\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657094fb",
   "metadata": {},
   "source": [
    "## Ignore warnings and set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ca3b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0b1f9",
   "metadata": {},
   "source": [
    "## ElectricityDemandForecaster Class: Modular Implementation\n",
    "\n",
    "The following cells define the `ElectricityDemandForecaster` class in a modular way. Each function is presented in its own cell, with explanations and usage tips. This structure makes the notebook easy to navigate, test, and extend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9adeb5d",
   "metadata": {},
   "source": [
    "### Class Initialization\n",
    "\n",
    "Define the `ElectricityDemandForecaster` class and its constructor. This sets up the main attributes and prepares the environment for forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14782f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectricityDemandForecaster:\n",
    "    \"\"\"\n",
    "    A class to handle electricity demand forecasting using multiple models and ensemble techniques.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path=None, forecast_horizon=24):\n",
    "        self.data_path = data_path\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.df = None\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.feature_columns = None\n",
    "        self.target_column = 'demand'\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "        self.models = {}\n",
    "        self.predictions = {}\n",
    "        self.metrics = {}\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    def load_data(self, data_path=None):\n",
    "        if data_path:\n",
    "            self.data_path = data_path\n",
    "        print(f\"Loading data from: {self.data_path}\")\n",
    "        self.df = pd.read_csv(self.data_path)\n",
    "        missing_values = self.df.isnull().sum()\n",
    "        print(\"\\nMissing values in each column:\")\n",
    "        print(missing_values)\n",
    "        print(\"\\nDataset info:\")\n",
    "        print(f\"Number of rows: {self.df.shape[0]}\")\n",
    "        print(f\"Number of columns: {self.df.shape[1]}\")\n",
    "        print(\"\\nColumn names:\")\n",
    "        print(self.df.columns.tolist())\n",
    "        return self.df\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data by converting timestamps, extracting features, and scaling.\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
    "        # Convert timestamp column to datetime if not already\n",
    "        if not np.issubdtype(self.df['timestamp'].dtype, np.datetime64):\n",
    "            self.df['timestamp'] = pd.to_datetime(self.df['timestamp'])\n",
    "        # Sort by timestamp\n",
    "        self.df = self.df.sort_values('timestamp').reset_index(drop=True)\n",
    "        # Feature engineering: extract time-based features\n",
    "        self.df['hour'] = self.df['timestamp'].dt.hour\n",
    "        self.df['dayofweek'] = self.df['timestamp'].dt.dayofweek\n",
    "        self.df['month'] = self.df['timestamp'].dt.month\n",
    "        self.df['is_weekend'] = self.df['dayofweek'].isin([5,6]).astype(int)\n",
    "        # Fill missing values (if any) with forward fill\n",
    "        self.df = self.df.fillna(method='ffill')\n",
    "        # Set feature columns (excluding timestamp and target)\n",
    "        self.feature_columns = [col for col in self.df.columns if col not in ['timestamp', self.target_column]]\n",
    "        # Scale features and target\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        self.df[self.feature_columns] = self.feature_scaler.fit_transform(self.df[self.feature_columns])\n",
    "        self.df[self.target_column] = self.target_scaler.fit_transform(self.df[[self.target_column]])\n",
    "        return self.df\n",
    "\n",
    "    def split_train_test(self, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Split the data into training and testing sets.\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not loaded or preprocessed.\")\n",
    "        n_test = int(len(self.df) * test_size)\n",
    "        self.train_df = self.df.iloc[:-n_test]\n",
    "        self.test_df = self.df.iloc[-n_test:]\n",
    "        self.X_train = self.train_df[self.feature_columns]\n",
    "        self.y_train = self.train_df[self.target_column]\n",
    "        self.X_test = self.test_df[self.feature_columns]\n",
    "        self.y_test = self.test_df[self.target_column]\n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "\n",
    "    def implement_naive_forecast(self):\n",
    "        \"\"\"\n",
    "        Implement a naive forecast model using the value from 24 hours ago.\n",
    "        \"\"\"\n",
    "        if self.test_df is None:\n",
    "            raise ValueError(\"Data not split. Call split_train_test() first.\")\n",
    "        # Naive forecast: previous day's value (24 hours ago)\n",
    "        horizon = self.forecast_horizon\n",
    "        y_pred = self.test_df[self.target_column].shift(horizon).fillna(method='bfill')\n",
    "        y_true = self.test_df[self.target_column]\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "        self.predictions['naive'] = y_pred.values\n",
    "        self.metrics['naive'] = {'mae': mae, 'rmse': rmse, 'mape': mape}\n",
    "        print(f\"Naive Forecast - MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}\")\n",
    "        return y_pred\n",
    "\n",
    "    def create_features_targets(self, data, lag_hours=24, window_size=7*24):\n",
    "        \"\"\"\n",
    "        Create a dataset with lagged features for time series forecasting.\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        for lag in range(1, lag_hours+1):\n",
    "            df[f'lag_{lag}'] = df[self.target_column].shift(lag)\n",
    "        df['rolling_mean'] = df[self.target_column].rolling(window=window_size).mean()\n",
    "        df['rolling_std'] = df[self.target_column].rolling(window=window_size).std()\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "        feature_cols = [col for col in df.columns if col not in ['timestamp', self.target_column]]\n",
    "        X = df[feature_cols]\n",
    "        y = df[self.target_column]\n",
    "        return X, y\n",
    "\n",
    "    def implement_xgboost(self):\n",
    "        \"\"\"\n",
    "        Implement an XGBoost model for time series forecasting.\n",
    "        \"\"\"\n",
    "        X_train, y_train = self.X_train, self.y_train\n",
    "        X_test, y_test = self.X_test, self.y_test\n",
    "        model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        self.models['xgboost'] = model\n",
    "        self.predictions['xgboost'] = y_pred\n",
    "        self.metrics['xgboost'] = {'mae': mae, 'rmse': rmse, 'mape': mape}\n",
    "        print(f\"XGBoost - MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}\")\n",
    "        return y_pred\n",
    "\n",
    "    def create_sequences(self, data, seq_length):\n",
    "        \"\"\"\n",
    "        Create sequences for LSTM model.\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            X.append(data[i:i+seq_length, :])\n",
    "            y.append(data[i+seq_length, 0])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def implement_lstm(self, seq_length=24, epochs=30, batch_size=64):\n",
    "        \"\"\"\n",
    "        Implement an LSTM model for time series forecasting.\n",
    "        \"\"\"\n",
    "        # Prepare data for LSTM\n",
    "        data = np.hstack([self.X_train.values, self.y_train.values.reshape(-1,1)])\n",
    "        X_lstm, y_lstm = self.create_sequences(data, seq_length)\n",
    "        data_test = np.hstack([self.X_test.values, self.y_test.values.reshape(-1,1)])\n",
    "        X_test_lstm, y_test_lstm = self.create_sequences(data_test, seq_length)\n",
    "        # Model\n",
    "        model = Sequential([\n",
    "            LSTM(64, input_shape=(seq_length, X_lstm.shape[2]), return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            LSTM(32),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mae')\n",
    "        es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        model.fit(X_lstm, y_lstm, validation_split=0.1, epochs=epochs, batch_size=batch_size, callbacks=[es], verbose=0)\n",
    "        y_pred = model.predict(X_test_lstm).flatten()\n",
    "        mae = mean_absolute_error(y_test_lstm, y_pred)\n",
    "        rmse = math.sqrt(mean_squared_error(y_test_lstm, y_pred))\n",
    "        mape = mean_absolute_percentage_error(y_test_lstm, y_pred)\n",
    "        self.models['lstm'] = model\n",
    "        self.predictions['lstm'] = y_pred\n",
    "        self.metrics['lstm'] = {'mae': mae, 'rmse': rmse, 'mape': mape}\n",
    "        print(f\"LSTM - MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}\")\n",
    "        return y_pred\n",
    "\n",
    "    def create_ensemble(self, weights_options=None):\n",
    "        \"\"\"\n",
    "        Create an ensemble model by combining XGBoost and LSTM predictions.\n",
    "        \"\"\"\n",
    "        if 'xgboost' not in self.predictions or 'lstm' not in self.predictions:\n",
    "            raise ValueError(\"Run both XGBoost and LSTM models first.\")\n",
    "        xgb_pred = self.predictions['xgboost']\n",
    "        lstm_pred = self.predictions['lstm']\n",
    "        y_true = self.y_test[-len(xgb_pred):] if len(self.y_test) > len(xgb_pred) else self.y_test\n",
    "        if weights_options is None:\n",
    "            weights_options = [(0.5, 0.5), (0.6, 0.4), (0.4, 0.6)]\n",
    "        best_mae = float('inf')\n",
    "        best_weights = None\n",
    "        best_pred = None\n",
    "        for w_xgb, w_lstm in weights_options:\n",
    "            ensemble_pred = w_xgb * xgb_pred + w_lstm * lstm_pred\n",
    "            mae = mean_absolute_error(y_true, ensemble_pred)\n",
    "            if mae < best_mae:\n",
    "                best_mae = mae\n",
    "                best_weights = (w_xgb, w_lstm)\n",
    "                best_pred = ensemble_pred\n",
    "        self.predictions['ensemble'] = best_pred\n",
    "        self.metrics['ensemble'] = {'mae': best_mae, 'weights': best_weights}\n",
    "        print(f\"Ensemble - MAE: {best_mae:.4f}, Weights: {best_weights}\")\n",
    "        return best_pred\n",
    "\n",
    "    def compare_models(self):\n",
    "        \"\"\"\n",
    "        Compare all implemented models and visualize results.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for model_name, metric in self.metrics.items():\n",
    "            results.append({'Model': model_name, 'MAE': metric['mae'], 'RMSE': metric.get('rmse', None), 'MAPE': metric.get('mape', None)})\n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(results_df)\n",
    "        plt.figure(figsize=(8,4))\n",
    "        sns.barplot(x='Model', y='MAE', data=results_df)\n",
    "        plt.title('Model MAE Comparison')\n",
    "        plt.show()\n",
    "        return results_df\n",
    "\n",
    "    def save_models(self, output_dir='models'):\n",
    "        \"\"\"\n",
    "        Save all models for future use.\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        if 'xgboost' in self.models:\n",
    "            joblib.dump(self.models['xgboost'], os.path.join(output_dir, 'xgboost_demand_forecasting.pkl'))\n",
    "        if 'lstm' in self.models:\n",
    "            self.models['lstm'].save(os.path.join(output_dir, 'lstm_demand_forecasting.keras'))\n",
    "        if hasattr(self, 'target_scaler'):\n",
    "            joblib.dump(self.target_scaler, os.path.join(output_dir, 'target_scaler.pkl'))\n",
    "        if 'ensemble' in self.metrics:\n",
    "            with open(os.path.join(output_dir, 'ensemble_weights.json'), 'w') as f:\n",
    "                json.dump({'weights': self.metrics['ensemble']['weights']}, f)\n",
    "        print(f\"Models saved to {output_dir}\")\n",
    "\n",
    "    def save_results(self, output_dir='results'):\n",
    "        \"\"\"\n",
    "        Save model metrics, logs, and comparison results to JSON and CSV files.\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(os.path.join(output_dir, 'model_metrics.json'), 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "        comparison_df = pd.DataFrame([{**{'model': k}, **v} for k, v in self.metrics.items()])\n",
    "        comparison_df.to_csv(os.path.join(output_dir, 'model_comparison.csv'), index=False)\n",
    "        log = {'timestamp': datetime.now().isoformat(), 'metrics': self.metrics}\n",
    "        with open(os.path.join(output_dir, f'training_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.json'), 'w') as f:\n",
    "            json.dump(log, f, indent=2)\n",
    "        print(f\"Results saved to {output_dir}\")\n",
    "\n",
    "    def get_best_model(self):\n",
    "        \"\"\"\n",
    "        Determine the best model based on MAE metric.\n",
    "        \"\"\"\n",
    "        best_model = min(self.metrics.items(), key=lambda x: x[1]['mae'])[0]\n",
    "        print(f\"Best model: {best_model}\")\n",
    "        return best_model\n",
    "\n",
    "    def visualize_results(self, sample_days=7):\n",
    "        \"\"\"\n",
    "        Visualize forecasting results for all models.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(15,6))\n",
    "        n = sample_days * 24\n",
    "        y_true = self.y_test[:n]\n",
    "        plt.plot(range(n), y_true, label='Actual', color='black')\n",
    "        for model_name, y_pred in self.predictions.items():\n",
    "            plt.plot(range(n), y_pred[:n], label=model_name.capitalize())\n",
    "        plt.legend()\n",
    "        plt.title(f'Forecasting Results for {sample_days} Days')\n",
    "        plt.xlabel('Hour')\n",
    "        plt.ylabel('Scaled Demand')\n",
    "        plt.show()\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the complete forecasting pipeline.\n",
    "        \"\"\"\n",
    "        self.load_data()\n",
    "        self.preprocess_data()\n",
    "        self.split_train_test()\n",
    "        self.implement_naive_forecast()\n",
    "        self.implement_xgboost()\n",
    "        self.implement_lstm()\n",
    "        self.create_ensemble()\n",
    "        self.compare_models()\n",
    "        self.visualize_results()\n",
    "        self.save_models()\n",
    "        self.save_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
